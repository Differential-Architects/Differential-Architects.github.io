<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">

<!-- jQuery library -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

<!-- Popper JS -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>

<!-- Latest compiled JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="navbarstyles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="imageclassification.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/>
    <title>RPA ML Labs - ML</title>
</head>
<body style="background-image: url(patterns.jpg); background-repeat:  no-repeat; background-size: cover; ">

<div class="container-fluid">
    <p class="banner"> Image Correction Model based on Generative Adversial Networks.</p> <br>
    <p class="banner2"> Time to get our hands dirty.</p>
    <div class="container">
        <div class="jumbotron">
                <button class="heading collapsible">Background - The "What"</button>
                    <div class="content">
                            <br>
                            <p class="subheading"> What are Generative Adversial Networks (GAN)? </p>
                            <p class="text"> 
                                Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks.
                                
                                Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.
                                
                                GANs are a clever way of training a generative model by framing the problem as a supervised learning problem with two sub-models: the generator model that we train to generate new examples, and the discriminator model that tries to classify examples as either real (from the domain) or fake (generated). The two models are trained together in a zero-sum game, adversarial, until the discriminator model is fooled about half the time, meaning the generator model is generating plausible examples.     
                            </p>
                            <img class="img-fluid" src="ANN.gif"> <br><br>
                            <p class="text">Neural networks are a set of algorithms, whose model is inspired by the human brain. Neural networks are designed to recognise and distinguish between patterns. They interpret sensory data through a kind of machine perception, labelling or clustering raw input. The patterns they recognise are numerical, contained in vectors, into which data such as images, sound, or text must be translated.</p>
                            <p class="subheading"> That's all cool, but how do GANs work? </p>
                            <img class="img-fluid mx-auto d-block" src="gann.jpg"> <br><br>
                            <p class="text">
                                The GAN model architecture involves two sub-models: a generator model for generating new examples and a discriminator model for classifying whether generated examples are real, from the domain, or fake, generated by the generator model.
                            <ul class="text">
                                <li><strong> Generator: </strong> Model that is used to generate new plausible examples from the problem domain.</li>
                                <li><strong> Discriminator: </strong> Model that is used to classify examples as real (from the domain) or fake (generated).</li>
                            </ul>
                            </p>
                            <p class="subheading"> The Generator Model </p>
                            <p class="text"> 
                                The generator model takes a fixed-length random vector as input and generates a sample in the domain.

                                The vector is drawn from randomly from a Gaussian distribution, and the vector is used to seed the generative process. After training, points in this multidimensional vector space will correspond to points in the problem domain, forming a compressed representation of the data distribution.
                                
                                This vector space is referred to as a latent space, or a vector space comprised of latent variables. Latent variables, or hidden variables, are those variables that are important for a domain but are not directly observable.
                            </p>
                            <img class="img-fluid mx-auto d-block" src="generator.png"> <br><br>
                            <p class="subheading"> The Discriminator Model </p>
                            <p class="text"> 
                                The discriminator model takes an example from the domain as input (real or generated) and predicts a binary class label of real or fake (generated).

                                The real example comes from the training dataset. The generated examples are output by the generator model.
                                
                                The discriminator is a normal (and well understood) classification model.
                                
                                After the training process, the discriminator model is discarded as we are interested in the generator.
                            </p>
                            <img class="img-fluid mx-auto d-block" src="discriminator.png"> <br><br>
                            <p class="subheading"> Understanding how both play as a two player game </p>
                            <p class="text"> 
                                We can think of the generator as being like a counterfeiter, trying to make fake money, and the discriminator as being like police, trying to allow legitimate money and catch counterfeit money. To succeed in this game, the counterfeiter must learn to make money that is indistinguishable from genuine money, and the generator network must learn to create samples that are drawn from the same distribution as the training data.
                            </p>
                            <br>
                            <p class="text">
                                Generative modeling is an unsupervised learning problem, as we discussed in the previous section, although a clever property of the GAN architecture is that the training of the generative model is framed as a supervised learning problem.

                                The two models, the generator and discriminator, are trained together. The generator generates a batch of samples, and these, along with real examples from the domain, are provided to the discriminator and classified as real or fake.
                                
                                The discriminator is then updated to get better at discriminating real and fake samples in the next round, and importantly, the generator is updated based on how well, or not, the generated samples fooled the discriminator.
                            </p>
                            <img class="img-fluid mx-auto d-block" src="ganmodel.png"> <br><br>
                    </div>
                    <br><br>
                    <button class="heading collapsible">Code - The "How"</button>
                    <div class="content">
                            <br>
                            <p class="text"> We're going to start with a pre-trained model called GFP-GAN (Generative Facial Prior Generative Adversial Network) that aims at developing a Practical Algorithm for Real-world Face Restoration. It leverages rich and diverse priors encapsulated in a pretrained face GAN (e.g., StyleGAN2) for image correction</p>
                            <p class="subheading"> Step 1: Installing all the dependencies </p>
                            <p class="text">We're going to use Google Colab for this part so we can utilize its GPU.</p> <br>
                            <p class="text">Here's the code for that part.</p>
                            <pre><code>
                                # Clone GFPGAN and enter the GFPGAN folder
                                %cd /content
                                !rm -rf GFPGAN
                                !git clone https://github.com/TencentARC/GFPGAN.git
                                %cd GFPGAN

                                # Set up the environment
                                # Install basicsr - https://github.com/xinntao/BasicSR
                                # We use BasicSR for both training and inference
                                !pip install basicsr
                                # Install facexlib - https://github.com/xinntao/facexlib
                                # We use face detection and face restoration helper in the facexlib package
                                !pip install facexlib
                                # Install other depencencies
                                !pip install -r requirements.txt
                                !python setup.py develop
                                !pip install realesrgan  # used for enhancing the background (non-face) regions
                                # Download the pre-trained model
                                !wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models
                            </code></pre>
                            <br>
                            <p class="subheading"> Step 2: Uploading the inputs </p>
                            <p class="text">We can use the demo images or our own images from the internet for this purpose.</p>
                            <p class="text">Here's the code for using own images.</p>
                            <pre><code>
                                # upload your own images
                                import os
                                from google.colab import files
                                import shutil

                                upload_folder = 'inputs/upload'

                                if os.path.isdir(upload_folder):
                                    shutil.rmtree(upload_folder)
                                os.mkdir(upload_folder)

                                # upload images
                                uploaded = files.upload()
                                for filename in uploaded.keys():
                                dst_path = os.path.join(upload_folder, filename)
                                print(f'move {filename} to {dst_path}')
                                shutil.move(filename, dst_path)
                            </code></pre>
                            <p class="text">Or, you can use this code for the demo images.</p>
                            <pre><code>
                                import shutil
                                import os
                                upload_folder = 'inputs/upload'
                                
                                if os.path.isdir(upload_folder):
                                    shutil.rmtree(upload_folder)
                                os.makedirs(upload_folder, exist_ok=True)
                                shutil.move('inputs/whole_imgs/Blake_Lively.jpg', 'inputs/upload/Blake_Lively.jpg')
                            </code></pre>
                            <br>
                            <p class="subheading"> Step 3: Inference from pre-trained models </p>
                            <p class="text">Now we use the GFPGAN to restore the above low-quality images & We use [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN) for enhancing the background (non-face) regions</p>
                            <pre><code>
                                !rm -rf results
                                !python inference_gfpgan.py --upscale 2 --test_path inputs/upload --save_root results --model_path experiments/pretrained_models/GFPGANCleanv1-NoCE-C2.pth --bg_upsampler realesrgan
                                
                                !ls results/cmp
                            </code></pre>
                            <br><br>
                            <p class="subheading"> Step 4: Visualization of the Images </p>
                            <p class="text">Now we first visualize the cropped faces by displaying each of the images from the folder and create an output folder to store the same. We use the popular Open CV for the visualization. </p>
                            <pre><code>
                                import cv2
                                import matplotlib.pyplot as plt
                                def display(img1, img2):
                                  fig = plt.figure(figsize=(25, 10))
                                  ax1 = fig.add_subplot(1, 2, 1) 
                                  plt.title('Input image', fontsize=16)
                                  ax1.axis('off')
                                  ax2 = fig.add_subplot(1, 2, 2)
                                  plt.title('GFPGAN output', fontsize=16)
                                  ax2.axis('off')
                                  ax1.imshow(img1)
                                  ax2.imshow(img2)
                                def imread(img_path):
                                  img = cv2.imread(img_path)
                                  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                                  return img
                                
                                # display each image in the upload folder
                                import os
                                import glob
                                
                                input_folder = 'results/cropped_faces'
                                result_folder = 'results/restored_faces'
                                input_list = sorted(glob.glob(os.path.join(input_folder, '*')))
                                output_list = sorted(glob.glob(os.path.join(result_folder, '*')))
                                for input_path, output_path in zip(input_list, output_list):
                                  img_input = imread(input_path)
                                  img_output = imread(output_path)
                                  display(img_input, img_output)
                            </code></pre>
                            <img class="img-fluid mx-auto d-block" src="imgcor.png"> <br><br>
                            <br><br>
                            <p class="subheading"> Step 5: Visualization of the Image as a whole </p>
                            <pre><code>
                                import cv2
                                import matplotlib.pyplot as plt
                                def display(img1, img2):
                                  fig = plt.figure(figsize=(25, 10))
                                  ax1 = fig.add_subplot(1, 2, 1) 
                                  plt.title('Input image', fontsize=16)
                                  ax1.axis('off')
                                  ax2 = fig.add_subplot(1, 2, 2)
                                  plt.title('GFPGAN output', fontsize=16)
                                  ax2.axis('off')
                                  ax1.imshow(img1)
                                  ax2.imshow(img2)
                                def imread(img_path):
                                  img = cv2.imread(img_path)
                                  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                                  return img
                                
                                # display each image in the upload folder
                                import os
                                import glob
                                
                                input_folder = 'inputs/upload'
                                result_folder = 'results/restored_imgs'
                                input_list = sorted(glob.glob(os.path.join(input_folder, '*')))
                                output_list = sorted(glob.glob(os.path.join(result_folder, '*')))
                                for input_path, output_path in zip(input_list, output_list):
                                  img_input = imread(input_path)
                                  img_output = imread(output_path)
                                  display(img_input, img_output)
                            </code></pre>
                            <img class="img-fluid mx-auto d-block" src="imgcor2.png"> <br><br>
                            <br><br>
                            <p class="subheading"> Step 6: Saving the outputs </p>
                            <br>
                            <p class="text">Finally, we save the images to an output folder.</p>
                            <pre>
                                <code>
                                    !ls results
                                    print('Download results')
                                    os.system('zip -r download.zip results')
                                    files.download("download.zip")
                                </code>
                            </pre>
                            <p class="text">AND, that's a <strong>WRAP!</strong></p> <br>
                            <p class="text">Hope you had fun & learnt something. Browse around our lab for more.</p> <br>
                    </div>
        </div>
    </div>
</div>
<script>
    var coll = document.getElementsByClassName("collapsible");
    var i;
    
    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.maxHeight){
          content.style.maxHeight = null;
        } else {
          content.style.maxHeight = content.scrollHeight + "px";
        } 
      });
    }
    </script>
</body>
</html>